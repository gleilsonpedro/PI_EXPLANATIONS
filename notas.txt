-gerar contagem de features relevantes, somente para a classe 0
-testar com outros datasets 
- validar as explicacões para todos os datasets
- inserir graficos - facilitar a interpretação da explicação

Acuracia= 
Verdadeiros Positivos (VP)+Verdadeiros Negativos (VN) / Falsos Positivos (FP)+Falsos Negativos (FN) Verdadeiros Positivos(VP)+Verdadeiros Negativos (VN)
​
A sensibilidade (também chamada de recall) mede a proporção de exemplos positivos que foram corretamente identificados pelo modelo. É calculada como:

Sensibilidade = Verdadeiros Positivos (VP) / Verdadeiros Positivos (VP) + Falsos Negativos (FN)

​A sensibilidade indica o quão bem o modelo identifica a classe positiva.

É especialmente importante em problemas onde os falsos negativos (casos positivos classificados erroneamente como negativos) são críticos. Por exemplo, em diagnósticos médicos, um falso negativo pode ser muito perigoso.

- reduzir dimensdionalidade dos datasets grandes
- criar código removendo as features desnecessárias segundo a pi-explicacões para teste.



## NO ARTIGO O CÁLCULO DOS DELTAS É
"δ_j = (valor_atual - valor_extremo) * w_j"


Classe	Peso (w)	Valor Típico	Fórmula Delta
0	w > 0	Alto	(valor - min) * w
0	w < 0	Baixo	(valor - max) * w
1	w > 0	Baixo	(max - valor) * w	← Invertido!
1	w < 0	Alto	(min - valor) * w	← Invertido!


O que a PI-explicação mostra:

Para instâncias da classe 0: mostra as features que garantem que é dessa classe específica

Para instâncias da classe 1: mostra as features que excluem a possibilidade de ser classe 0






########## IDEIAS TEMAS

Comparação da pi exp´licação com outras técnicas (SHAP e LIME)
    Mostrar as vantagens das explicações em comparação com as outras
    Comparar a complexidade das explicações
    




    ***** pegar média das explicações

    opção de rejeição - adaptando o algoritimo com a minima.

    Explaining Logistic Regression with Reject Option in Loglinear Time


    pegar tamanho de cada explicação e fazer a média das explicações , (padrão)

    explicação - comparar com ancors
    opçcao de rejeição tem que ver pois o limiar tem que ser encontrado


    **********************************
    estava errado o calculo do R que no caso é o ΦR
        R = sum(delta) - gamma_A
    no artigo equação 14 mostra:
        phiR = gamma_A - Gamma_Omega ->  ΦR = Γa - Γω
        "onde Gamma_omega é o pior caso da função de decisão e phi é o threshold inicial passado
        pela função de decisão"

        ENTENDENDO: A função subtrai cada δj (valor da feature ordenado) de ΦR até que ΦR ≤ 0
        while ΦR ≥ 0:
            ΦR = ΦR - ∆[Idx]
            Xpl.add(feature_j)


