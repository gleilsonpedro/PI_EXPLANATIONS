-gerar contagem de features relevantes, somente para a classe 0
-testar com outros datasets 
- validar as explicacões para todos os datasets
- inserir graficos - facilitar a interpretação da explicação

Acuracia= 
Verdadeiros Positivos (VP)+Verdadeiros Negativos (VN) / Falsos Positivos (FP)+Falsos Negativos (FN) Verdadeiros Positivos(VP)+Verdadeiros Negativos (VN)
​
A sensibilidade (também chamada de recall) mede a proporção de exemplos positivos que foram corretamente identificados pelo modelo. É calculada como:

Sensibilidade = Verdadeiros Positivos (VP) / Verdadeiros Positivos (VP) + Falsos Negativos (FN)

​A sensibilidade indica o quão bem o modelo identifica a classe positiva.

É especialmente importante em problemas onde os falsos negativos (casos positivos classificados erroneamente como negativos) são críticos. Por exemplo, em diagnósticos médicos, um falso negativo pode ser muito perigoso.

- reduzir dimensdionalidade dos datasets grandes
- criar código removendo as features desnecessárias segundo a pi-explicacões para teste.



## NO ARTIGO O CÁLCULO DOS DELTAS É
"δ_j = (valor_atual - valor_extremo) * w_j"


Classe	Peso (w)	Valor Típico	Fórmula Delta
0	w > 0	Alto	(valor - min) * w
0	w < 0	Baixo	(valor - max) * w
1	w > 0	Baixo	(max - valor) * w	← Invertido!
1	w < 0	Alto	(min - valor) * w	← Invertido!


O que a PI-explicação mostra:

Para instâncias da classe 0: mostra as features que garantem que é dessa classe específica

Para instâncias da classe 1: mostra as features que excluem a possibilidade de ser classe 0






########## IDEIAS TEMAS

Comparação da pi exp´licação com outras técnicas (SHAP e LIME)
    Mostrar as vantagens das explicações em comparação com as outras
    Comparar a complexidade das explicações
    




    ***** pegar média das explicações

    opção de rejeição - adaptando o algoritimo com a minima.

    Explaining Logistic Regression with Reject Option in Loglinear Time


    pegar tamanho de cada explicação e fazer a média das explicações , (padrão)

    explicação - comparar com ancors
    opçcao de rejeição tem que ver pois o limiar tem que ser encontrado


    **********************************
    estava errado o calculo do R que no caso é o ΦR
        R = sum(delta) - gamma_A
    no artigo equação 14 mostra:
        phiR = gamma_A - Gamma_Omega ->  ΦR = Γa - Γω
        "onde Gamma_omega é o pior caso da função de decisão e phi é o threshold inicial passado
        pela função de decisão"

        ENTENDENDO: A função subtrai cada δj (valor da feature ordenado) de ΦR até que ΦR ≤ 0
        while ΦR ≥ 0:
            ΦR = ΦR - ∆[Idx]
            Xpl.add(feature_j)

        * o R mede a "folga" entre a instância atual (gamma_A) e o pior caso.
        * Critério de Parada: Quando Phi ≤ 0, as features já incluídas são suficientes para justificar a predição, mesmo que as demais features assumam seus piores valores.

            
*********************
sobre a regressão loghistica foi modificado:
- a função de decisão foi alterada para a função de decisão da regressão logA regularização padrão (com C=1.0) pode gerar pesos pequenos → gamma_A e delta pequenos → R ≈ 0 ou negativo.

Com C=10.0, o modelo tem mais liberdade para atribuir pesos maiores às features realmente importantes.

Isso vai aumentar os valores de gamma_A - gamma_omega = R, facilitando que o one_explanation() consiga encontrar uma explicação antes de esgotar a lista de features.

**********************

o gamma_A os calculos dele e do gamma_omega estão sendo  calculados com base na predição do modelo e não no rótulo verdadeiro como fala no artigo onde :

	gamma_A = modelo.decision_function([instancia])[0]
    gamma_omega = calcular_gamma_omega(modelo, X, y, y_test[idx])

    deve ficar assim:

        gamma_A = modelo.decision_function([instancia])[0]
        classe_predita = int(modelo.predict([instancia])[0])
        gamma_omega = calcular_gamma_omega(modelo, X, y, classe_predita)

        e a chamada no main fica : 

        explicacao = one_explanation(Vs, delta, R, feature_names, classe_0_nome, classe_1_nome, classe_predita)

        O ARTIGO - "We generate explanations with respect to the predicted class, not necessarily the true label"

        A PI-explicação mostra por que o modelo decidiu o que decidiu, não se ele estava certo ou não.

###### o que quero printar:
    Dataset e classes analisadas
    Acurácia
    Instância + classe predita
    Nº de features usadas
    PI-explicação
    Estatísticas (média, desvio)
    Distribuição das classes
    Features mais frequentes
